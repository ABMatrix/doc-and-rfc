# 从`ipfs add`到`ipfs get`之间到底发生了什么

## `ipfs add`
对于普通文件来说，add操作首先将文件切分成较为小的数据块（chunk--原始数据），
对于每一个块，ipfs会对其进行哈希计算，哈希算法默认是SHA256。
每一块的大小默认是256K。这些块会被解析成为IPFS块结构(ProtoNode)，
这个结构体里面较为重要的两个域一个是links，另一个是data。
这些块结构会随后被用来构建默克尔树。
不难理解的是，当节点为空的时候，links域为空数组，
data域里面保存原始数据以及结构体本身的一些固定数据
（具体内容不重要，但能被protobuf解析器捕捉到）；
而当节点为非叶子结点的时候，
links域保存的是子结点的哈希，
而data域只有结构体本身数据。

上面提到的links域里面保存子结点的哈希，
那么子结点哈希又是怎么计算得到的呢？
很简单，如果子结点是叶子结点，
links域里面每个哈希是根据叶子结点里的文件原始数据算出来的哈希；
但如果子结点不是叶子结点，那么该节点的哈希值是通过对ProtoNode结构体进行Marshal后算出来。

所有的ProtoNode会被解析成cid-raw_data对存入本地的blockstore里面。
这个blockstore类似于一个本地键值缓存数据库，关键字是文件哈希CID，
内容为根据ProtoNode解析出来的源数据。当然这一过程本身是一个构建默克尔树的过程，
期间会有自动查重，也就是说解析为相同哈希的数据块不会被二次存入blockstore里面。

默认情况下的接下来步骤就是固定（pin）文件并广播CID。
广播CID这一过程可以被认为是在该节点自己维护的分布式哈希表（DHT）里存一份记录，
关键字是该节点的PeerID，值为CID。有了这一步，
在其他节点上运行`ipfs dht findprovs <cid>`以后可以看到该节点的Peer ID出现在命令行的输出。

好了，广播过程相对简单易懂，那么这个固定又是什么样的骚操作呢？
其原理其实也不难理解。
首先固定器维护几个索引器（indexer）：直接索引器（dIndexer -- direct indexer），
递归索引器（rIndexer -- recursive indexer），
名字索引器（nameIndexer）。
其实无论是哪个索引器你都可以将其理解为数据库里的一张表（table），
又或者可以被理解成建立于blockstore之上的索引。
里面存放什么呢，当然是CID，只是每个CID有不同的固定类型。
有了这些索引为了干什么呢，
就是防止IPFS的垃圾回收器（garbage collector）在触发运行的时候不回收掉能在这些索引器里找到的CID。
这样就达到了永久保存用户想要永久保存的数据（块）的目的。

上面讲到的是对**普通文件**进行的“上传”操作，对于**文件夹**类型数据进行上传可以被理解为一个递归操作。这里需要注意的是，对于根文件夹而言，其ProtoNode里面对文件夹名本身没有做记录，因为默认情况下IPFS不会在根文件夹之上再建一个节点指向根文件夹，而这一点本身也应用于普通文件。当然如果你想IPFS也为你提供了一个`-w`选项，就是wrap的意思。言归正传，除了递归，在我们上述我们提到过每个ProtoNode结构体里有一个links域，这个域是一种`Link`数组，每个`Link`本身又是一个结构体。这个结构体里有`Size`,`Name`,`CID`这几个域。Size和CID这两个域就不用多赘述，想想也知道代表什么。而Name就是每个子文件名或者子文件夹名。我们知道IPFS通过CID索引文件，但有时候也可以通过_CID+文件路径_的形式获取相应的文件。比如说你上传了一整个文件夹，解析得到的跟文件夹的CID为`QmCID`，该文件夹下有一个子文件夹`foo`，这个子文件夹下有一个`bar`文件，你可以通过`ipfs get /ipfs/QmCID/foo/bar`获取到`bar`文件。

## `ipfs get`
`ipfs get`这一步操作在触发后会先对文件（夹）进行解析已验证参数是否有效。
如果有效会得到目标文件（夹）的CID。
拿到解析后的CID之后会去检索本地的blockstore看是否有对应的块早就已经存在本地上，如果是就返回将块里的文件内容写入指定的路径下。
这一步相对容易，但如果本地blockstore上没有相应的块呢？

这时候就要引入大名鼎鼎的bitswap协议。
这个协议怎么运作呢？
首先，本地机器会向全网广播一个`want_list`，一开始这个`want_list`里只有根CID。
随后存有该根CID下block的节点（们）会陆陆续续返回`have_list`。
理想状态下第一个回合结束want_list会被更新到被解析后的一系列可以组成根CID的子CID，
以及拥有该CID的Peer ID。
每次本地节点会根据上一轮从各同伴节点中获取的块数来计算下一轮从各同伴节点里获取块的概率；
比如在上一轮中A向“我”成功发送`b`块，B向“我”成功发送`2b`块，那么下一次有`1/3`的概率“我”会向A索取块，
剩下的`2/3`的概率“我”会向B索取块。

你以为这么快bitswap就玩完了吗？并没有，好戏在后头。

我向人索取而不反赠，这一点听上去好像有点不道德。
在计算机领域里也讲究个**礼尚往来**；也就是说，我今天给你送“礼”，你日后也要送我一些。
但我们知道，搞IT的人更喜欢大家一起分享，
也就是说我向某人索取以后并不可能立马有东西_回礼_给对方，
但我可以通过为全网贡献我刚拿到的数据来降低内心的罪恶感。
这也正好是bitswap的设计。
每个IPFS节点都维护一本bitswap账本，用来记载自己收到多少（好处）以及自己送给别人多少（好处）。
节点每次收到新块的时候会将块缓存到本地blockstore但默认不会固定这些块；
也就是说这些块如果不加固会被垃圾回收。
同时完成存入缓存又会向全网广播自己可以提供这些块，
这时候就有了`have_list`这一张表，专门用来广播自己所能提供的块。

当所有的块被写入本地blockstore以后便会开始将这些块“组装”成文件（夹）本身该有的模样，这点没什么难度。